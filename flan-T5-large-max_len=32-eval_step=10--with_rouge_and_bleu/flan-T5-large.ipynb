{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9866261f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: wandb in /home/pod/.local/lib/python3.10/site-packages (0.19.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/pod/.local/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/pod/.local/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.1.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.24.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /home/pod/.local/lib/python3.10/site-packages (from wandb) (2.10.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/pod/.local/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/pod/.local/lib/python3.10/site-packages (from wandb) (2.17.0)\n",
      "Requirement already satisfied: setproctitle in /home/pod/.local/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/pod/.local/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/pod/.local/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/pod/.local/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/pod/.local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: evaluate in /home/pod/.local/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/pod/.local/lib/python3.10/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.24.4)\n",
      "Requirement already satisfied: dill in /home/pod/.local/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/pod/.local/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/pod/.local/lib/python3.10/site-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/pod/.local/lib/python3.10/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/pod/.local/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.12.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/pod/.local/lib/python3.10/site-packages (from evaluate) (0.26.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/pod/.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/pod/.local/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: peft==0.12.0 in /home/pod/.local/lib/python3.10/site-packages (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.12.0) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.12.0) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.12.0) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.12.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.12.0) (2.2.0a0+81ea7a4)\n",
      "Requirement already satisfied: transformers in /home/pod/.local/lib/python3.10/site-packages (from peft==0.12.0) (4.47.0)\n",
      "Requirement already satisfied: tqdm in /home/pod/.local/lib/python3.10/site-packages (from peft==0.12.0) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/pod/.local/lib/python3.10/site-packages (from peft==0.12.0) (1.0.1)\n",
      "Requirement already satisfied: safetensors in /home/pod/.local/lib/python3.10/site-packages (from peft==0.12.0) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /home/pod/.local/lib/python3.10/site-packages (from peft==0.12.0) (0.26.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (2023.12.2)\n",
      "Requirement already satisfied: requests in /home/pod/.local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/pod/.local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.12.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.12.0) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.12.0) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.12.0) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/pod/.local/lib/python3.10/site-packages (from transformers->peft==0.12.0) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.12.0) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.12.0) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: bitsandbytes in /home/pod/.local/lib/python3.10/site-packages (0.45.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.2.0a0+81ea7a4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.24.4)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in /home/pod/.local/lib/python3.10/site-packages (from bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: sacrebleu in /home/pod/.local/lib/python3.10/site-packages (2.4.3)\n",
      "Requirement already satisfied: portalocker in /home/pod/.local/lib/python3.10/site-packages (from sacrebleu) (2.10.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.12.25)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.24.4)\n",
      "Requirement already satisfied: colorama in /home/pod/.local/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /home/pod/.local/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: transformers in /home/pod/.local/lib/python3.10/site-packages (4.47.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/pod/.local/lib/python3.10/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/pod/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/pod/.local/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/pod/.local/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/pod/.local/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/pod/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: datasets in /home/pod/.local/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/pod/.local/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/pod/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/pod/.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/pod/.local/lib/python3.10/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/pod/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/pod/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.12.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/pod/.local/lib/python3.10/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/pod/.local/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: rouge_score in /home/pod/.local/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /home/pod/.local/lib/python3.10/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.24.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /home/pod/.local/lib/python3.10/site-packages (from nltk->rouge_score) (4.66.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -U wandb\n",
    "! pip install evaluate\n",
    "! pip install peft==0.12.0\n",
    "! pip install -U bitsandbytes\n",
    "! pip install sacrebleu\n",
    "! pip install -U transformers\n",
    "! pip install datasets\n",
    "! pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ead3160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pod/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the latest cached version of the dataset since fixed_train couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/pod/.cache/huggingface/datasets/fixed_train/default/0.0.0/757978637cc23bf4 (last modified on Fri Dec  6 16:55:25 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'source_locale', 'target_locale', 'source', 'target', 'entities', 'from'],\n",
      "        num_rows: 32962\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'source_locale', 'target_locale', 'source', 'target', 'entities', 'from'],\n",
      "    num_rows: 29665\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'source_locale', 'target_locale', 'source', 'target', 'entities', 'from'],\n",
      "    num_rows: 3297\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"]=\"https://hf-mirror.com\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, \\\n",
    "    Seq2SeqTrainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import wandb\n",
    "from  datasets import Dataset\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"fixed_train\") \n",
    "\n",
    "print(dataset)\n",
    "# 32962\n",
    "train_val = dataset['train'].train_test_split(test_size=0.1)\n",
    "train_data = train_val['train']\n",
    "print(train_data)\n",
    "# 29665\n",
    "val_data = train_val['test']\n",
    "print(val_data)\n",
    "# 3297"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "995d1d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to read the token file at /var/run/secrets/kubernetes.io/serviceaccount/token due to permission error ([Errno 13] Permission denied: '/var/run/secrets/kubernetes.io/serviceaccount/token').The current user id is 1000. Consider changing the securityContext to run the container as the current user.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to read the token file at /var/run/secrets/kubernetes.io/serviceaccount/token due to permission error ([Errno 13] Permission denied: '/var/run/secrets/kubernetes.io/serviceaccount/token').The current user id is 1000. Consider changing the securityContext to run the container as the current user.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskyfurynowonline\u001b[0m (\u001b[33mskyfurynowonline-yunnan-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/pod/semeval2025/flan-T5-large/wandb/run-20241206_165649-cw7lamj9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skyfurynowonline-yunnan-university/flan-T5-large-max_len%3D32/runs/cw7lamj9' target=\"_blank\">ruby-terrain-1</a></strong> to <a href='https://wandb.ai/skyfurynowonline-yunnan-university/flan-T5-large-max_len%3D32' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/skyfurynowonline-yunnan-university/flan-T5-large-max_len%3D32' target=\"_blank\">https://wandb.ai/skyfurynowonline-yunnan-university/flan-T5-large-max_len%3D32</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/skyfurynowonline-yunnan-university/flan-T5-large-max_len%3D32/runs/cw7lamj9' target=\"_blank\">https://wandb.ai/skyfurynowonline-yunnan-university/flan-T5-large-max_len%3D32/runs/cw7lamj9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    os.environ['WANDB_API_KEY'] = \"a464ce6c3b972e3e7090ac20839b9a1daac1b608\"\n",
    "    os.environ[\"WANDB_PROJECT\"] = \"flan-T5-large-max_len=32\"\n",
    "    wandb.init()\n",
    "    \n",
    "    # 尝试移除无用列\n",
    "    column_name = train_data.column_names\n",
    "\n",
    "    model_id = \"google/flan-t5-large\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "#     prefix = '''Given a input sentence from the source language (en) that contains named entities, your task is to accurately translate the named entities in the input sentence to the target language {}: '''\n",
    "    prefix = \"translate English to {}: \"\n",
    "    # metrics\n",
    "    bleuM = evaluate.load(\"sacrebleu\")\n",
    "    rougeM = evaluate.load(\"rouge\")\n",
    "    \n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "    # 使用data_collator将数据集中的多个样本合并成一个batch，对齐输入和标签的长度\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,model=model)\n",
    "    def preprocess_function(examples):\n",
    "#         '''\n",
    "#         在需要处理一个批次样本时，可以通过设置 batched=True，让 map 传入一组样本的批次。\n",
    "#         此时，自定义函数的参数会变成一个包含每列字段的字典，字段的值为一个列表，代表该批次中所有样本在对应列下的数据。\n",
    "#         这样，用户可以灵活选择是否逐条或批量处理数据。\n",
    "#         :param examples:\n",
    "#         :return:\n",
    "#         '''\n",
    "#         inputs = [prefix.format(target_lcoale)+example for example,target_lcoale in zip(examples['source'],examples['target_locale'])]\n",
    "        translate_dict={\n",
    "            \"ar\":\"Arabic\",\n",
    "            \"de\":\"German\",\n",
    "            \"es\":\"Spanish\",\n",
    "            \"fr\":\"French\",\n",
    "            \"it\":\"Italian\",\n",
    "            \"ja\":\"Japanese\",\n",
    "        }\n",
    "        inputs = [prefix.format(translate_dict[target_lcoale])+example for example,target_lcoale in zip(examples['source'],examples['target_locale'])]\n",
    "        \n",
    "        \n",
    "        targets = examples['target']\n",
    "        \n",
    "        \n",
    "#         model_inputs = tokenizer(inputs,text_target=targets,max_length=50,truncation=True,padding=True,return_tensors=\"pt\")\n",
    "\n",
    "        model_inputs = tokenizer(inputs, max_length=32, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        labels = tokenizer(targets, max_length=32, truncation=True, padding=\"max_length\",return_tensors='pt')\n",
    "\n",
    "        # 将目标文本的填充 token 设置为 -100，以便在计算损失时忽略\n",
    "        labels[\"input_ids\"] = [\n",
    "            [l if l != tokenizer.pad_token_id else -100 for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "        # 将编码后的目标设置为模型的标签\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "       \n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a471e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'source_locale', 'target_locale', 'source', 'target', 'entities', 'from'],\n",
      "    num_rows: 29665\n",
      "})\n",
      "{'id': 'd2081455', 'source_locale': 'en', 'target_locale': 'ar', 'source': 'What is the tallest mountain in the largest state in the U.S.?', 'target': 'ما هو أطول جبل في أكبر ولاية في الولايات المتحدة؟', 'entities': ['Q30'], 'from': 'mintaka'}\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "# 29665\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b261ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 29665/29665 [00:22<00:00, 1336.86 examples/s]\n",
      "Map: 100%|██████████| 3297/3297 [00:02<00:00, 1257.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "    tokenized_train = train_data.map(preprocess_function,batched=True,remove_columns=column_name)\n",
    "    tokenized_val = val_data.map(preprocess_function,batched=True,remove_columns=column_name)\n",
    "    # 使用一部分进行验证\n",
    "#     tokenized_val_slice = Dataset.from_dict(tokenized_val[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b74ae20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pod/.local/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1403/1994667227.py:80: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44499' max='44499' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44499/44499 5:23:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.036400</td>\n",
       "      <td>0.827691</td>\n",
       "      <td>0.356938</td>\n",
       "      <td>0.228335</td>\n",
       "      <td>0.353424</td>\n",
       "      <td>0.353149</td>\n",
       "      <td>24.401473</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.935900</td>\n",
       "      <td>0.770691</td>\n",
       "      <td>0.373198</td>\n",
       "      <td>0.231525</td>\n",
       "      <td>0.369517</td>\n",
       "      <td>0.369252</td>\n",
       "      <td>24.965055</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.865700</td>\n",
       "      <td>0.731789</td>\n",
       "      <td>0.375760</td>\n",
       "      <td>0.241958</td>\n",
       "      <td>0.371918</td>\n",
       "      <td>0.372026</td>\n",
       "      <td>26.165507</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.850800</td>\n",
       "      <td>0.707813</td>\n",
       "      <td>0.369421</td>\n",
       "      <td>0.241434</td>\n",
       "      <td>0.365668</td>\n",
       "      <td>0.365377</td>\n",
       "      <td>25.488899</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.793300</td>\n",
       "      <td>0.692244</td>\n",
       "      <td>0.384312</td>\n",
       "      <td>0.250587</td>\n",
       "      <td>0.380725</td>\n",
       "      <td>0.380318</td>\n",
       "      <td>27.199919</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.779400</td>\n",
       "      <td>0.676008</td>\n",
       "      <td>0.391602</td>\n",
       "      <td>0.254628</td>\n",
       "      <td>0.388139</td>\n",
       "      <td>0.387724</td>\n",
       "      <td>27.696821</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.751900</td>\n",
       "      <td>0.662714</td>\n",
       "      <td>0.387108</td>\n",
       "      <td>0.255139</td>\n",
       "      <td>0.383428</td>\n",
       "      <td>0.383284</td>\n",
       "      <td>27.622594</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.797800</td>\n",
       "      <td>0.641695</td>\n",
       "      <td>0.388351</td>\n",
       "      <td>0.257240</td>\n",
       "      <td>0.384588</td>\n",
       "      <td>0.384221</td>\n",
       "      <td>28.118522</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.731600</td>\n",
       "      <td>0.640550</td>\n",
       "      <td>0.385487</td>\n",
       "      <td>0.256606</td>\n",
       "      <td>0.381919</td>\n",
       "      <td>0.381613</td>\n",
       "      <td>26.930255</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.756900</td>\n",
       "      <td>0.625460</td>\n",
       "      <td>0.389032</td>\n",
       "      <td>0.261599</td>\n",
       "      <td>0.385898</td>\n",
       "      <td>0.385523</td>\n",
       "      <td>26.859089</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.743900</td>\n",
       "      <td>0.629853</td>\n",
       "      <td>0.387396</td>\n",
       "      <td>0.264374</td>\n",
       "      <td>0.383960</td>\n",
       "      <td>0.383849</td>\n",
       "      <td>27.145655</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.696900</td>\n",
       "      <td>0.622195</td>\n",
       "      <td>0.384914</td>\n",
       "      <td>0.260924</td>\n",
       "      <td>0.381628</td>\n",
       "      <td>0.381477</td>\n",
       "      <td>26.790223</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.716100</td>\n",
       "      <td>0.605647</td>\n",
       "      <td>0.400255</td>\n",
       "      <td>0.268839</td>\n",
       "      <td>0.397298</td>\n",
       "      <td>0.397001</td>\n",
       "      <td>28.140549</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.707400</td>\n",
       "      <td>0.607812</td>\n",
       "      <td>0.398491</td>\n",
       "      <td>0.270083</td>\n",
       "      <td>0.394980</td>\n",
       "      <td>0.394760</td>\n",
       "      <td>28.497892</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.684600</td>\n",
       "      <td>0.596237</td>\n",
       "      <td>0.395073</td>\n",
       "      <td>0.268358</td>\n",
       "      <td>0.392059</td>\n",
       "      <td>0.391797</td>\n",
       "      <td>27.625546</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.690600</td>\n",
       "      <td>0.590100</td>\n",
       "      <td>0.397306</td>\n",
       "      <td>0.270226</td>\n",
       "      <td>0.394268</td>\n",
       "      <td>0.394011</td>\n",
       "      <td>28.384354</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.708400</td>\n",
       "      <td>0.583570</td>\n",
       "      <td>0.394949</td>\n",
       "      <td>0.271204</td>\n",
       "      <td>0.391802</td>\n",
       "      <td>0.391565</td>\n",
       "      <td>28.697092</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.680100</td>\n",
       "      <td>0.587192</td>\n",
       "      <td>0.399655</td>\n",
       "      <td>0.271407</td>\n",
       "      <td>0.396472</td>\n",
       "      <td>0.396135</td>\n",
       "      <td>28.227538</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.697900</td>\n",
       "      <td>0.579403</td>\n",
       "      <td>0.394893</td>\n",
       "      <td>0.271627</td>\n",
       "      <td>0.392205</td>\n",
       "      <td>0.392023</td>\n",
       "      <td>28.166364</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.567649</td>\n",
       "      <td>0.396445</td>\n",
       "      <td>0.272013</td>\n",
       "      <td>0.393502</td>\n",
       "      <td>0.393160</td>\n",
       "      <td>27.633072</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.660300</td>\n",
       "      <td>0.567186</td>\n",
       "      <td>0.399518</td>\n",
       "      <td>0.276379</td>\n",
       "      <td>0.396272</td>\n",
       "      <td>0.396098</td>\n",
       "      <td>29.371463</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.613600</td>\n",
       "      <td>0.567788</td>\n",
       "      <td>0.403249</td>\n",
       "      <td>0.276661</td>\n",
       "      <td>0.400098</td>\n",
       "      <td>0.399993</td>\n",
       "      <td>28.704761</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.564362</td>\n",
       "      <td>0.405818</td>\n",
       "      <td>0.282119</td>\n",
       "      <td>0.403109</td>\n",
       "      <td>0.402706</td>\n",
       "      <td>29.076157</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.651700</td>\n",
       "      <td>0.562552</td>\n",
       "      <td>0.408654</td>\n",
       "      <td>0.281461</td>\n",
       "      <td>0.405723</td>\n",
       "      <td>0.405571</td>\n",
       "      <td>28.823071</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.616900</td>\n",
       "      <td>0.552444</td>\n",
       "      <td>0.400894</td>\n",
       "      <td>0.281749</td>\n",
       "      <td>0.398100</td>\n",
       "      <td>0.397678</td>\n",
       "      <td>29.378449</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.655600</td>\n",
       "      <td>0.550527</td>\n",
       "      <td>0.400919</td>\n",
       "      <td>0.282844</td>\n",
       "      <td>0.398112</td>\n",
       "      <td>0.397500</td>\n",
       "      <td>29.040438</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.640500</td>\n",
       "      <td>0.552103</td>\n",
       "      <td>0.402422</td>\n",
       "      <td>0.280428</td>\n",
       "      <td>0.399825</td>\n",
       "      <td>0.399344</td>\n",
       "      <td>28.868566</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.632400</td>\n",
       "      <td>0.546339</td>\n",
       "      <td>0.410326</td>\n",
       "      <td>0.283216</td>\n",
       "      <td>0.407469</td>\n",
       "      <td>0.407083</td>\n",
       "      <td>29.268125</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.605100</td>\n",
       "      <td>0.547611</td>\n",
       "      <td>0.408653</td>\n",
       "      <td>0.283399</td>\n",
       "      <td>0.405877</td>\n",
       "      <td>0.405705</td>\n",
       "      <td>29.212425</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.597000</td>\n",
       "      <td>0.545823</td>\n",
       "      <td>0.414159</td>\n",
       "      <td>0.286427</td>\n",
       "      <td>0.411443</td>\n",
       "      <td>0.411155</td>\n",
       "      <td>29.690557</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.566800</td>\n",
       "      <td>0.539974</td>\n",
       "      <td>0.407156</td>\n",
       "      <td>0.286895</td>\n",
       "      <td>0.404631</td>\n",
       "      <td>0.404257</td>\n",
       "      <td>29.539037</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.575300</td>\n",
       "      <td>0.539672</td>\n",
       "      <td>0.403604</td>\n",
       "      <td>0.285291</td>\n",
       "      <td>0.400783</td>\n",
       "      <td>0.400478</td>\n",
       "      <td>29.528215</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.549600</td>\n",
       "      <td>0.539398</td>\n",
       "      <td>0.410512</td>\n",
       "      <td>0.284111</td>\n",
       "      <td>0.408123</td>\n",
       "      <td>0.407727</td>\n",
       "      <td>29.127995</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.540900</td>\n",
       "      <td>0.537426</td>\n",
       "      <td>0.407858</td>\n",
       "      <td>0.284379</td>\n",
       "      <td>0.405420</td>\n",
       "      <td>0.405014</td>\n",
       "      <td>28.998739</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.544200</td>\n",
       "      <td>0.540015</td>\n",
       "      <td>0.409559</td>\n",
       "      <td>0.284234</td>\n",
       "      <td>0.407162</td>\n",
       "      <td>0.406691</td>\n",
       "      <td>28.617386</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.532100</td>\n",
       "      <td>0.542622</td>\n",
       "      <td>0.407810</td>\n",
       "      <td>0.284942</td>\n",
       "      <td>0.405451</td>\n",
       "      <td>0.405080</td>\n",
       "      <td>29.011193</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.572100</td>\n",
       "      <td>0.527752</td>\n",
       "      <td>0.413158</td>\n",
       "      <td>0.285029</td>\n",
       "      <td>0.410929</td>\n",
       "      <td>0.410165</td>\n",
       "      <td>29.509529</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.531827</td>\n",
       "      <td>0.410961</td>\n",
       "      <td>0.287941</td>\n",
       "      <td>0.408624</td>\n",
       "      <td>0.408109</td>\n",
       "      <td>29.614622</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.555600</td>\n",
       "      <td>0.521003</td>\n",
       "      <td>0.409850</td>\n",
       "      <td>0.286814</td>\n",
       "      <td>0.407547</td>\n",
       "      <td>0.407163</td>\n",
       "      <td>29.259698</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>0.534982</td>\n",
       "      <td>0.413245</td>\n",
       "      <td>0.288439</td>\n",
       "      <td>0.410836</td>\n",
       "      <td>0.410682</td>\n",
       "      <td>29.524871</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.558300</td>\n",
       "      <td>0.522246</td>\n",
       "      <td>0.409301</td>\n",
       "      <td>0.286682</td>\n",
       "      <td>0.407127</td>\n",
       "      <td>0.406842</td>\n",
       "      <td>29.463820</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.534600</td>\n",
       "      <td>0.530310</td>\n",
       "      <td>0.416670</td>\n",
       "      <td>0.286676</td>\n",
       "      <td>0.413980</td>\n",
       "      <td>0.413692</td>\n",
       "      <td>29.540424</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.540900</td>\n",
       "      <td>0.526789</td>\n",
       "      <td>0.414735</td>\n",
       "      <td>0.288890</td>\n",
       "      <td>0.412063</td>\n",
       "      <td>0.411802</td>\n",
       "      <td>29.580653</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.531400</td>\n",
       "      <td>0.521158</td>\n",
       "      <td>0.413860</td>\n",
       "      <td>0.289803</td>\n",
       "      <td>0.411266</td>\n",
       "      <td>0.410643</td>\n",
       "      <td>29.884411</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.570100</td>\n",
       "      <td>0.517171</td>\n",
       "      <td>0.419451</td>\n",
       "      <td>0.290236</td>\n",
       "      <td>0.417199</td>\n",
       "      <td>0.416543</td>\n",
       "      <td>30.296293</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.522300</td>\n",
       "      <td>0.518553</td>\n",
       "      <td>0.414967</td>\n",
       "      <td>0.289240</td>\n",
       "      <td>0.412484</td>\n",
       "      <td>0.411969</td>\n",
       "      <td>29.955060</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.495300</td>\n",
       "      <td>0.517478</td>\n",
       "      <td>0.416141</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.413423</td>\n",
       "      <td>0.413098</td>\n",
       "      <td>29.896764</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.551800</td>\n",
       "      <td>0.509216</td>\n",
       "      <td>0.418911</td>\n",
       "      <td>0.291205</td>\n",
       "      <td>0.416507</td>\n",
       "      <td>0.416225</td>\n",
       "      <td>30.073495</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.544500</td>\n",
       "      <td>0.512964</td>\n",
       "      <td>0.421673</td>\n",
       "      <td>0.293574</td>\n",
       "      <td>0.418756</td>\n",
       "      <td>0.418556</td>\n",
       "      <td>30.670684</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.541800</td>\n",
       "      <td>0.519806</td>\n",
       "      <td>0.420823</td>\n",
       "      <td>0.292596</td>\n",
       "      <td>0.418211</td>\n",
       "      <td>0.418080</td>\n",
       "      <td>30.648018</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.549800</td>\n",
       "      <td>0.515418</td>\n",
       "      <td>0.419541</td>\n",
       "      <td>0.291586</td>\n",
       "      <td>0.416937</td>\n",
       "      <td>0.416682</td>\n",
       "      <td>30.335757</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.502300</td>\n",
       "      <td>0.513738</td>\n",
       "      <td>0.420522</td>\n",
       "      <td>0.293356</td>\n",
       "      <td>0.417884</td>\n",
       "      <td>0.417469</td>\n",
       "      <td>30.361330</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.536600</td>\n",
       "      <td>0.501485</td>\n",
       "      <td>0.421202</td>\n",
       "      <td>0.296672</td>\n",
       "      <td>0.418682</td>\n",
       "      <td>0.418244</td>\n",
       "      <td>30.810399</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.507600</td>\n",
       "      <td>0.515148</td>\n",
       "      <td>0.422793</td>\n",
       "      <td>0.297004</td>\n",
       "      <td>0.420155</td>\n",
       "      <td>0.419939</td>\n",
       "      <td>31.017483</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.537000</td>\n",
       "      <td>0.516778</td>\n",
       "      <td>0.422416</td>\n",
       "      <td>0.296228</td>\n",
       "      <td>0.419794</td>\n",
       "      <td>0.419362</td>\n",
       "      <td>30.835230</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.514500</td>\n",
       "      <td>0.524020</td>\n",
       "      <td>0.424718</td>\n",
       "      <td>0.296624</td>\n",
       "      <td>0.422263</td>\n",
       "      <td>0.421774</td>\n",
       "      <td>30.861886</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.536300</td>\n",
       "      <td>0.505651</td>\n",
       "      <td>0.423154</td>\n",
       "      <td>0.297270</td>\n",
       "      <td>0.420524</td>\n",
       "      <td>0.420074</td>\n",
       "      <td>30.656740</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>0.511238</td>\n",
       "      <td>0.421001</td>\n",
       "      <td>0.295563</td>\n",
       "      <td>0.418433</td>\n",
       "      <td>0.417956</td>\n",
       "      <td>30.629739</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.533300</td>\n",
       "      <td>0.505925</td>\n",
       "      <td>0.419960</td>\n",
       "      <td>0.296116</td>\n",
       "      <td>0.417218</td>\n",
       "      <td>0.416627</td>\n",
       "      <td>30.615186</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.482300</td>\n",
       "      <td>0.507688</td>\n",
       "      <td>0.419574</td>\n",
       "      <td>0.297413</td>\n",
       "      <td>0.416780</td>\n",
       "      <td>0.416445</td>\n",
       "      <td>30.981284</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.475500</td>\n",
       "      <td>0.508556</td>\n",
       "      <td>0.421720</td>\n",
       "      <td>0.297676</td>\n",
       "      <td>0.419145</td>\n",
       "      <td>0.418807</td>\n",
       "      <td>31.015917</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.513800</td>\n",
       "      <td>0.512856</td>\n",
       "      <td>0.424393</td>\n",
       "      <td>0.298315</td>\n",
       "      <td>0.421583</td>\n",
       "      <td>0.421205</td>\n",
       "      <td>30.917754</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.455600</td>\n",
       "      <td>0.511380</td>\n",
       "      <td>0.421054</td>\n",
       "      <td>0.295946</td>\n",
       "      <td>0.418240</td>\n",
       "      <td>0.417984</td>\n",
       "      <td>30.689430</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.509524</td>\n",
       "      <td>0.422920</td>\n",
       "      <td>0.295767</td>\n",
       "      <td>0.420172</td>\n",
       "      <td>0.419977</td>\n",
       "      <td>30.562757</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.499950</td>\n",
       "      <td>0.423421</td>\n",
       "      <td>0.296556</td>\n",
       "      <td>0.420679</td>\n",
       "      <td>0.420247</td>\n",
       "      <td>30.798102</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.487000</td>\n",
       "      <td>0.507874</td>\n",
       "      <td>0.422067</td>\n",
       "      <td>0.296594</td>\n",
       "      <td>0.419362</td>\n",
       "      <td>0.419014</td>\n",
       "      <td>30.779561</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.456800</td>\n",
       "      <td>0.508954</td>\n",
       "      <td>0.422201</td>\n",
       "      <td>0.296265</td>\n",
       "      <td>0.419395</td>\n",
       "      <td>0.419166</td>\n",
       "      <td>30.760674</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.491300</td>\n",
       "      <td>0.506218</td>\n",
       "      <td>0.422399</td>\n",
       "      <td>0.296584</td>\n",
       "      <td>0.419590</td>\n",
       "      <td>0.419242</td>\n",
       "      <td>30.835253</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.489900</td>\n",
       "      <td>0.505873</td>\n",
       "      <td>0.421915</td>\n",
       "      <td>0.297002</td>\n",
       "      <td>0.419341</td>\n",
       "      <td>0.418973</td>\n",
       "      <td>30.906771</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.481400</td>\n",
       "      <td>0.512555</td>\n",
       "      <td>0.423492</td>\n",
       "      <td>0.297672</td>\n",
       "      <td>0.420812</td>\n",
       "      <td>0.420419</td>\n",
       "      <td>31.118186</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.452300</td>\n",
       "      <td>0.499890</td>\n",
       "      <td>0.423947</td>\n",
       "      <td>0.297027</td>\n",
       "      <td>0.421043</td>\n",
       "      <td>0.420700</td>\n",
       "      <td>30.872761</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.477900</td>\n",
       "      <td>0.494164</td>\n",
       "      <td>0.424878</td>\n",
       "      <td>0.298551</td>\n",
       "      <td>0.422279</td>\n",
       "      <td>0.421954</td>\n",
       "      <td>31.114596</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.484900</td>\n",
       "      <td>0.505551</td>\n",
       "      <td>0.423554</td>\n",
       "      <td>0.297285</td>\n",
       "      <td>0.420877</td>\n",
       "      <td>0.420528</td>\n",
       "      <td>30.975839</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.491500</td>\n",
       "      <td>0.498434</td>\n",
       "      <td>0.425188</td>\n",
       "      <td>0.298317</td>\n",
       "      <td>0.422625</td>\n",
       "      <td>0.422214</td>\n",
       "      <td>30.951252</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>0.509461</td>\n",
       "      <td>0.421845</td>\n",
       "      <td>0.296741</td>\n",
       "      <td>0.419180</td>\n",
       "      <td>0.418783</td>\n",
       "      <td>30.836458</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.467200</td>\n",
       "      <td>0.502689</td>\n",
       "      <td>0.423131</td>\n",
       "      <td>0.296421</td>\n",
       "      <td>0.420398</td>\n",
       "      <td>0.420154</td>\n",
       "      <td>30.793975</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.474700</td>\n",
       "      <td>0.493936</td>\n",
       "      <td>0.425732</td>\n",
       "      <td>0.298373</td>\n",
       "      <td>0.423334</td>\n",
       "      <td>0.422947</td>\n",
       "      <td>30.938217</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.473500</td>\n",
       "      <td>0.498343</td>\n",
       "      <td>0.423066</td>\n",
       "      <td>0.298453</td>\n",
       "      <td>0.420384</td>\n",
       "      <td>0.420093</td>\n",
       "      <td>30.966843</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.436800</td>\n",
       "      <td>0.508127</td>\n",
       "      <td>0.422055</td>\n",
       "      <td>0.298867</td>\n",
       "      <td>0.419265</td>\n",
       "      <td>0.419219</td>\n",
       "      <td>30.930851</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.481100</td>\n",
       "      <td>0.504418</td>\n",
       "      <td>0.423113</td>\n",
       "      <td>0.299077</td>\n",
       "      <td>0.420563</td>\n",
       "      <td>0.420073</td>\n",
       "      <td>31.010324</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.480100</td>\n",
       "      <td>0.505810</td>\n",
       "      <td>0.422855</td>\n",
       "      <td>0.298368</td>\n",
       "      <td>0.420194</td>\n",
       "      <td>0.419830</td>\n",
       "      <td>30.817041</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.464300</td>\n",
       "      <td>0.504889</td>\n",
       "      <td>0.421086</td>\n",
       "      <td>0.298296</td>\n",
       "      <td>0.418660</td>\n",
       "      <td>0.418190</td>\n",
       "      <td>30.824571</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>0.502131</td>\n",
       "      <td>0.422234</td>\n",
       "      <td>0.297969</td>\n",
       "      <td>0.419603</td>\n",
       "      <td>0.419253</td>\n",
       "      <td>30.919752</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.450300</td>\n",
       "      <td>0.504366</td>\n",
       "      <td>0.421915</td>\n",
       "      <td>0.298380</td>\n",
       "      <td>0.419526</td>\n",
       "      <td>0.419110</td>\n",
       "      <td>30.885130</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.484000</td>\n",
       "      <td>0.497155</td>\n",
       "      <td>0.422076</td>\n",
       "      <td>0.298226</td>\n",
       "      <td>0.419666</td>\n",
       "      <td>0.419481</td>\n",
       "      <td>30.914000</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.462200</td>\n",
       "      <td>0.502777</td>\n",
       "      <td>0.422841</td>\n",
       "      <td>0.298564</td>\n",
       "      <td>0.420635</td>\n",
       "      <td>0.420045</td>\n",
       "      <td>30.868504</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.501762</td>\n",
       "      <td>0.422939</td>\n",
       "      <td>0.298589</td>\n",
       "      <td>0.420395</td>\n",
       "      <td>0.420194</td>\n",
       "      <td>30.923495</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.429700</td>\n",
       "      <td>0.501832</td>\n",
       "      <td>0.423304</td>\n",
       "      <td>0.298646</td>\n",
       "      <td>0.420741</td>\n",
       "      <td>0.420559</td>\n",
       "      <td>30.964359</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>▁▂▄▄▃▄▅▅▄▅▆▆▆▆▆▆▆▆▆▇▇▇███▇██████████████</td></tr><tr><td>eval/gen_len</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>█▇▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/rouge1</td><td>▁▃▄▄▄▅▅▅▅▅▆▅▇▆▆▇▇▇▇█▇███▇███████████████</td></tr><tr><td>eval/rouge2</td><td>▁▂▃▄▄▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████</td></tr><tr><td>eval/rougeL</td><td>▁▃▃▄▅▄▄▄▄▅▅▆▆▆▆▇▆▆▆▇▆▇▇▇▇▇███▇██████████</td></tr><tr><td>eval/rougeLsum</td><td>▁▃▃▂▄▄▅▅▅▅▅▅▆▇▆▆▆▆▇▇▇▇█▇█▇▇▇████████████</td></tr><tr><td>eval/runtime</td><td>▇▆█▇▇█▇▇▇▆█▇▁▁▁▁▁▁▁▁▁▁▂▂▂▂▁▂▃▃▃▃▄▄▃▃▃▄▃▃</td></tr><tr><td>eval/samples_per_second</td><td>▁▂▁▁▂▁▁▁▁▁▁▆█▇██▇█▇██▇▇█▇▇▇▆▆▅▆▅▆▅▆▅▆▅▅▅</td></tr><tr><td>eval/steps_per_second</td><td>▂▃▁▂▂▂▂▂▁▃▂▂▂█▇███████▇▇▇▆▆▆▆▅▆▆▅▆▆▆▆▅▆▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▃▃▂▁▂▂▃▃▅▃▂▄▂▂▂▂▂▃▃█▄▂▂▂▃▃▃▄▃▃▂▂▂▅▃▂▁▂▁</td></tr><tr><td>train/learning_rate</td><td>█████▇▇▇▇▇▇▆▆▆▆▆▆▆▆▅▅▅▅▅▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▅▅▅▄▄▄▄▃▄▄▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▂▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>30.96436</td></tr><tr><td>eval/gen_len</td><td>32</td></tr><tr><td>eval/loss</td><td>0.50183</td></tr><tr><td>eval/rouge1</td><td>0.4233</td></tr><tr><td>eval/rouge2</td><td>0.29865</td></tr><tr><td>eval/rougeL</td><td>0.42074</td></tr><tr><td>eval/rougeLsum</td><td>0.42056</td></tr><tr><td>eval/runtime</td><td>69.7314</td></tr><tr><td>eval/samples_per_second</td><td>47.281</td></tr><tr><td>eval/steps_per_second</td><td>11.831</td></tr><tr><td>total_flos</td><td>1.281955148660736e+16</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>44499</td></tr><tr><td>train/grad_norm</td><td>1.63711</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4297</td></tr><tr><td>train_loss</td><td>0.57777</td></tr><tr><td>train_runtime</td><td>19387.3812</td></tr><tr><td>train_samples_per_second</td><td>4.59</td></tr><tr><td>train_steps_per_second</td><td>2.295</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ruby-terrain-1</strong> at: <a href='https://wandb.ai/skyfurynowonline-yunnan-university/flan-T5-large-max_len%3D32/runs/cw7lamj9' target=\"_blank\">https://wandb.ai/skyfurynowonline-yunnan-university/flan-T5-large-max_len%3D32/runs/cw7lamj9</a><br/> View project at: <a href='https://wandb.ai/skyfurynowonline-yunnan-university/flan-T5-large-max_len%3D32' target=\"_blank\">https://wandb.ai/skyfurynowonline-yunnan-university/flan-T5-large-max_len%3D32</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241206_165649-cw7lamj9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    def postprocess_text(preds, labels):\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "        labels = [[label.strip()] for label in labels]\n",
    "        return preds, labels\n",
    "    \n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        preds, labels = eval_pred\n",
    "        \n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "            \n",
    "#         print(preds.shape)\n",
    "#         batchsize,seq_len,vocab_size\n",
    "#         (100, 64, 32128)\n",
    "#         input()\n",
    "        \n",
    "        # 将 predictions 从概率转为 token IDs ?\n",
    "        preds = np.argmax(preds,axis=-1) # (batch_size, seq_len)\n",
    "        \n",
    "        # 需要额外处理label吗？\n",
    "        # transformers 中，填充位置通常被标记为 -100，需要将这些位置替换为 tokenizer.pad_token_id，避免在解码时生成错误文本\n",
    "        \n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        \n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "     \n",
    "        # 已经是批量了\n",
    "#         result1 = bleuM.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "#         result2 = rougeM.compute(predictions=decoded_preds,references=decoded_labels)\n",
    "        \n",
    "        \n",
    "        result = rougeM.compute(predictions=decoded_preds,references=decoded_labels)\n",
    "        reuslt_bleu = bleuM.compute(predictions=decoded_preds,references=decoded_labels)\n",
    "        result['bleu'] = reuslt_bleu['score']\n",
    "        \n",
    "#         result = {\n",
    "#             \"bleu\": result1[\"score\"],\n",
    "#             \"rouge\":result2['score'],\n",
    "#         }\n",
    "        \n",
    "        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "        result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "#         result = {k: round(v, 4) for k, v in result.items()}\n",
    "        return result\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir='./semeval-flan-T5-large-max_len=32',  # output directory\n",
    "        save_total_limit=3,       # 只保留最近的 3 个检查点\n",
    "#         report_to=\"wandb\",\n",
    "        num_train_epochs=3,  # total number of training epochs\n",
    "        per_device_train_batch_size=2,  # batch size per device during training\n",
    "        per_device_eval_batch_size=4,  # batch size for evaluation\n",
    "        warmup_steps=200,  # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,  # strength of weight decay\n",
    "        logging_dir='./logs',  # directory for storing logs\n",
    "        \n",
    "        learning_rate=2e-5,  # Set learning rate\n",
    "        evaluation_strategy=\"steps\",\n",
    "        \n",
    "        \n",
    "        # 频繁的日志记录可能导致内存增加，特别是当模型需要频繁评估时。\n",
    "        logging_steps=500,\n",
    "        eval_accumulation_steps=100,\n",
    "        \n",
    "        # 使用T5-base时遇到loss为NAN的问题，或许尝试不使用gradient_checkpointing\n",
    "        # 尝试禁用gradient_checkpointing和FP16，跑出了结果\n",
    "        \n",
    "        # 过多裁剪会导致不能学习到有用的知识\n",
    "#         max_grad_norm=1.0,  # 最大梯度裁剪，防止梯度爆炸\n",
    "        \n",
    "        save_strategy=\"epoch\",\n",
    "        \n",
    "        gradient_checkpointing=True,\n",
    "#         fp16=True, # 开启混合精度\n",
    "        # gradient_accumulation_steps=8,  # 累积 8 个小批次\n",
    "    )\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = tokenized_train,\n",
    "        eval_dataset = tokenized_val,\n",
    "        \n",
    "#         train_dataset = tokenized_train_slice,\n",
    "#         eval_dataset=tokenized_val_slice,\n",
    "        \n",
    "        tokenizer = tokenizer,\n",
    "        data_collator = data_collator,\n",
    "        compute_metrics = compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "#     trainer.evaluate()\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10b5e522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[13959,  1566,    12,  4338,    10,   363,   671,   467,   141,     8,\n",
      "           167,  1085,    16,  3581,    58,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "Quale videogioco ha avuto il maggior numero di venduti nel 2006?\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, \\\n",
    "    Seq2SeqTrainer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./semeval-flan-T5-large-max_len=32/checkpoint-44499\").to(\"cuda\")\n",
    "tokenizerSelf = AutoTokenizer.from_pretrained(\"./semeval-flan-T5-large-max_len=32/checkpoint-44499\")\n",
    "text = \"translate English to Italian: What video game had the most sales in 2006?\"\n",
    "# target：Quale videogioco ha registrato il maggior numero di vendite nel 2006?\n",
    "inputs = tokenizerSelf(text,return_tensors=\"pt\").to(\"cuda\")\n",
    "print(inputs)\n",
    "outputs = model.generate(inputs['input_ids'],max_length=64,num_beams=5,early_stopping=True)\n",
    "\n",
    "decoded_output = tokenizerSelf.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3214ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questo gioco di video è stato il più venduto nel 2006?\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, \\\n",
    "    Seq2SeqTrainer\n",
    "model_id = \"google/flan-t5-large\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# text = \"translate English to Japanese: Hello, how are you ? \"\n",
    "text = \"translate English to Italian: What video game had the most sales in 2006?\"\n",
    "inputs = tokenizer(text,return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(inputs['input_ids'],max_length=64,num_beams=5,early_stopping=True)\n",
    "\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f2be0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
